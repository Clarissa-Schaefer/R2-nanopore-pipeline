{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.SeqFeature import SeqFeature, FeatureLocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 (Filter 1) - Total records: 26386\n",
      "Passed Filter 1 (have flanking regions with combined directions >= 100bp): 10175\n",
      "Failed Filter 1 (no valid flanking regions or combined directions < 100bp): 16211\n",
      "\n",
      "Reasons for Discarding Reads:\n",
      " - Discarded due to failing combined length requirements: 15345\n",
      " - Discarded due to failing to find a best flanking pair: 866\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------\n",
    "# 1. Reference Parsing\n",
    "# ---------------------------------------------------\n",
    "from collections import defaultdict\n",
    "from Bio import SeqIO\n",
    "\n",
    "def parse_reference_fasta(ref_fasta):\n",
    "    \"\"\"\n",
    "    Read reference FASTA, parse known feature segments, return dict of feature lengths.\n",
    "    Assumes the reference or an accompanying table has boundaries or lengths of each feature.\n",
    "    \"\"\"\n",
    "    feature_lengths = {}\n",
    "  \n",
    "    for record in SeqIO.parse(ref_fasta, \"fasta\"):\n",
    "        feature_lengths[record.id] = len(record.seq)\n",
    "  \n",
    "    return feature_lengths\n",
    "\n",
    "ref_fasta_path = \"00-Data/references/ref_28s_features.fasta\"\n",
    "ref_lengths = parse_reference_fasta(ref_fasta_path)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2. Parse Read Reference Positions\n",
    "# ---------------------------------------------------\n",
    "# Store as (read_id, type) -> [(start, end, strand), ...]\n",
    "read_ref_positions = defaultdict(list)\n",
    "alignment_file = \"00-Data/repeatmaskered/sample-cutadapt_sup-filtered.fasta.out.xm\"\n",
    "\n",
    "with open(alignment_file) as infile:\n",
    "    for line_num, line in enumerate(infile, 1):\n",
    "        # Split the line on whitespace\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) < 10:\n",
    "            # Skip lines that don't have enough columns\n",
    "            print(f\"Skipping presumed header line {line_num}: {line}\")\n",
    "            continue  \n",
    "\n",
    "        # Extract the relevant information\n",
    "        read_id = parts[4]\n",
    "        strand = parts[8]\n",
    "        feat_type = parts[9].split(\"#\")[0]\n",
    "\n",
    "        # Extract start and end positions based on strand\n",
    "        try:\n",
    "            if strand == \"+\":\n",
    "                start = int(parts[10])\n",
    "                end = int(parts[11])\n",
    "            elif strand == \"C\":\n",
    "                start = int(parts[12])\n",
    "                end = int(parts[11])\n",
    "            else:\n",
    "                # Skip if strand information is invalid\n",
    "                print(f\"Skipping line {line_num}: invalid strand {strand}\")\n",
    "                continue  \n",
    "        except (IndexError, ValueError):\n",
    "            # Skip lines with invalid position data\n",
    "            print(f\"Skipping line {line_num}: invalid positions\")\n",
    "            continue  \n",
    "\n",
    "        read_ref_positions[(read_id, feat_type)].append((start, end, strand))\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3. Helper Function to Get Best Flanking Pair with Combined Length Checks\n",
    "# ---------------------------------------------------\n",
    "def get_best_flanking_pair(record, feat_3_names, feat_5_names):\n",
    "    \"\"\"\n",
    "    Return (best_3_feat, best_5_feat) or None if no valid pair is found.\n",
    "    'Best' means same-strand pair with the longest distance between them.\n",
    "    \"\"\"\n",
    "    three_features = []\n",
    "    five_features = []\n",
    "\n",
    "    # Gather 3end & 5end features\n",
    "    for feat in record.features:\n",
    "        if feat.type in feat_3_names:\n",
    "            three_features.append(feat)\n",
    "        elif feat.type in feat_5_names:\n",
    "            five_features.append(feat)\n",
    "    \n",
    "    # No 3 or 5 features -> no valid pair\n",
    "    if not three_features or not five_features:\n",
    "        return None\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # **New Addition**: Calculate sum of lengths per direction\n",
    "    # --------------------------------------------\n",
    "    # Define directions as (feature type, strand)\n",
    "    direction_sums = defaultdict(int)\n",
    "    for feat in three_features + five_features:\n",
    "        direction = (feat.type, feat.strand)\n",
    "        direction_sums[direction] += len(feat.location)\n",
    "\n",
    "    # Identify valid directions with sum >= 100 nt\n",
    "    valid_directions = set()\n",
    "    for direction, total_length in direction_sums.items():\n",
    "        if total_length >= 100:\n",
    "            valid_directions.add(direction)\n",
    "\n",
    "    # Filter features to keep only those in valid directions\n",
    "    filtered_three_features = [feat for feat in three_features if (feat.type, feat.strand) in valid_directions]\n",
    "    filtered_five_features = [feat for feat in five_features if (feat.type, feat.strand) in valid_directions]\n",
    "\n",
    "    # If no features remain after filtering, return None\n",
    "    if not filtered_three_features or not filtered_five_features:\n",
    "        return None\n",
    "\n",
    "    # Proceed to find the best pair from filtered features\n",
    "    best_pair = None\n",
    "    max_distance = -1\n",
    "\n",
    "    # Find pair with same strand & the greatest distance\n",
    "    for tfeat in filtered_three_features:\n",
    "        for ffeat in filtered_five_features:\n",
    "            if tfeat.strand == ffeat.strand and tfeat.strand in (1, -1):\n",
    "                left_coord = min(tfeat.location.start, ffeat.location.start)\n",
    "                right_coord = max(tfeat.location.end, ffeat.location.end)\n",
    "                distance = right_coord - left_coord\n",
    "                if distance > max_distance:\n",
    "                    max_distance = distance\n",
    "                    best_pair = (tfeat, ffeat)\n",
    "    \n",
    "    return best_pair  # could be None if no same-strand pairs\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4. Define Input Files and Feature Names\n",
    "# ---------------------------------------------------\n",
    "input_file = \"00-Data/feature_annotated/Repeatmasker_transgene.gb\"\n",
    "feat_3_names = [\"3end_200nt\", \"3end_400nt\"]  # adjust if needed\n",
    "feat_5_names = [\"5end_200nt\", \"5end_400nt\"]  # adjust if needed\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5. Apply Filter 1 and Classify Records with Detailed Failure Reasons\n",
    "# ---------------------------------------------------\n",
    "records_filter1_pass = []\n",
    "records_filter1_fail = []\n",
    "\n",
    "# Initialize counters (optional, for tracking purposes)\n",
    "counters = defaultdict(int)\n",
    "# Possible counters:\n",
    "# - 'discarded_missing_features': missing 3end or 5end features\n",
    "# - 'discarded_due_to_combined_length': failing combined length requirements\n",
    "# - 'discarded_due_to_flanking_pair': failing to find best flanking pair\n",
    "\n",
    "# Initialize lists to store read IDs that passed and failed\n",
    "passed_reads = []  # Not used\n",
    "failed_reads = []  # Not used\n",
    "\n",
    "# Initialize list for reads that failed due to combined length\n",
    "discarded_due_to_combined_length_reads = []\n",
    "\n",
    "# Read the input file and classify records based on Filter 1\n",
    "for record in SeqIO.parse(input_file, \"genbank\"):\n",
    "    best_pair = get_best_flanking_pair(record, feat_3_names, feat_5_names)\n",
    "    if best_pair is not None:\n",
    "        # Since combined lengths are already checked in get_best_flanking_pair,\n",
    "        # we no longer need to check individual feature lengths here.\n",
    "        records_filter1_pass.append(record)\n",
    "        passed_reads.append(record.id)\n",
    "    else:\n",
    "        # Determine the reason for failure\n",
    "        # Check if the failure was due to combined length or no valid pair\n",
    "        # To do this, we need to recalculate the combined lengths\n",
    "\n",
    "        # Gather all 3end & 5end features\n",
    "        three_features = [feat for feat in record.features if feat.type in feat_3_names]\n",
    "        five_features = [feat for feat in record.features if feat.type in feat_5_names]\n",
    "\n",
    "        # Calculate sum of lengths per direction\n",
    "        direction_sums = defaultdict(int)\n",
    "        for feat in three_features + five_features:\n",
    "            direction = (feat.type, feat.strand)\n",
    "            direction_sums[direction] += len(feat.location)\n",
    "\n",
    "        # Check if any direction meets the combined length requirement\n",
    "        any_combined_length_met = any(total >= 100 for total in direction_sums.values())\n",
    "\n",
    "        if not any_combined_length_met:\n",
    "            # Failed due to combined length requirements\n",
    "            counters['discarded_due_to_combined_length'] += 1\n",
    "            discarded_due_to_combined_length_reads.append(record.id)\n",
    "        else:\n",
    "            # Failed to find a valid flanking pair despite meeting combined length\n",
    "            counters['discarded_due_to_flanking_pair'] += 1\n",
    "\n",
    "        # Add to failed records\n",
    "        records_filter1_fail.append(record)\n",
    "        failed_reads.append(record.id)\n",
    "\n",
    "# Print out how many passed/failed Filter 1\n",
    "total_records = len(records_filter1_pass) + len(records_filter1_fail)\n",
    "print(f\"Step 1 (Filter 1) - Total records: {total_records}\")\n",
    "print(f\"Passed Filter 1 (have flanking regions with combined directions >= 100bp): {len(records_filter1_pass)}\")\n",
    "print(f\"Failed Filter 1 (no valid flanking regions or combined directions < 100bp): {len(records_filter1_fail)}\")\n",
    "\n",
    "# Print the number of reads discarded due to specific reasons\n",
    "print(\"\\nReasons for Discarding Reads:\")\n",
    "print(f\" - Discarded due to failing combined length requirements: {counters['discarded_due_to_combined_length']}\")\n",
    "print(f\" - Discarded due to failing to find a best flanking pair: {counters['discarded_due_to_flanking_pair']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2 (Filter 2) - Starting with 10175 Filter 1-passed records.\n",
      "Passed Filter 2 (contain Transgene or 3UTR): 846\n",
      "Failed Filter 2 (no Transgene or 3UTR): 9329\n",
      " - Discarded due to missing Transgene or 3UTR: 9329\n",
      "\n",
      "Step 3 (Filter 3) - Among 9329 records that have flanking regions but no Transgene or 3UTR:\n",
      "Number with ≥50 unannotated bases between flanking regions: 0\n",
      "Trimming done.\n",
      "Trimmed records: 846\n",
      "Removed records: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "######################################################\n",
    "#        FILTER 2 - From Filter 1, keep those with an\n",
    "#           insert feature (adjust feature name).\n",
    "######################################################\n",
    "\n",
    "def has_required_feature(record, feature_names=[\"Transgene\", \"3UTR\"]):\n",
    "    \"\"\"\n",
    "    Check if the record contains any feature with type in feature_names.\n",
    "    \n",
    "    Args:\n",
    "        record (SeqRecord): The sequence record to check.\n",
    "        feature_names (list): List of feature types to look for.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if any feature matches, False otherwise.\n",
    "    \"\"\"\n",
    "    return any(feat.type in feature_names for feat in record.features)\n",
    "\n",
    "# Initialize lists to store passed and failed records for Filter 2\n",
    "records_filter2_pass = []\n",
    "records_filter2_fail = []  # Records that have flanking region but neither Transgene nor 3UTR\n",
    "\n",
    "# Iterate over records that passed Filter 1\n",
    "for record in records_filter1_pass:\n",
    "    if has_required_feature(record, feature_names=[\"Transgene\", \"3UTR\"]):\n",
    "        records_filter2_pass.append(record)\n",
    "    else:\n",
    "        records_filter2_fail.append(record)\n",
    "        counters['discarded_due_to_missing_transgene_or_3UTR'] += 1\n",
    "\n",
    "# Output the results\n",
    "print(f\"\\nStep 2 (Filter 2) - Starting with {len(records_filter1_pass)} Filter 1-passed records.\")\n",
    "print(f\"Passed Filter 2 (contain Transgene or 3UTR): {len(records_filter2_pass)}\")\n",
    "print(f\"Failed Filter 2 (no Transgene or 3UTR): {len(records_filter2_fail)}\")\n",
    "\n",
    "# Print the number of reads discarded due to missing Transgene or 3UTR\n",
    "print(f\" - Discarded due to missing Transgene or 3UTR: {counters['discarded_due_to_missing_transgene_or_3UTR']}\")\n",
    "\n",
    "###################################################################################\n",
    "# # CELL 3: FILTER 3 - Among those that failed Filter 2 (but passed Filter 1),\n",
    "# #         check how many have ≥50 unannotated bases between the 3end and 5end.\n",
    "###################################################################################\n",
    "\n",
    "def unannotated_region_in_flanking_interval(record, feat_3_names, feat_5_names, min_gap=50):\n",
    "    \"\"\"\n",
    "    Returns True if there is a region of >= min_gap bases between\n",
    "    the flanking features that is NOT overlapped by any other feature.\n",
    "    We use the 'best' 3end-5end pair from get_best_flanking_pair().\n",
    "    \"\"\"\n",
    "    best_pair = get_best_flanking_pair(record, feat_3_names, feat_5_names)\n",
    "    if not best_pair:\n",
    "        return False  # no flanking pair\n",
    "\n",
    "    three_feat, five_feat = best_pair\n",
    "    left = min(three_feat.location.end, five_feat.location.end)\n",
    "    right = max(three_feat.location.start, five_feat.location.start)\n",
    "    # The 'internal region' is left..right, if left < right.\n",
    "    # If left >= right, it means the features overlap or are adjacent.\n",
    "    \n",
    "    if left >= right:\n",
    "        return False\n",
    "    \n",
    "    gap_length = right - left\n",
    "    if gap_length < min_gap:\n",
    "        return False\n",
    "    \n",
    "    # Now we check if ANY feature overlaps that region\n",
    "    for feat in record.features:\n",
    "        f_start = feat.location.start\n",
    "        f_end = feat.location.end\n",
    "        # If a feature overlaps the gap region, it's annotated\n",
    "        if f_start < right and f_end > left:\n",
    "            return False  # Gap region is at least partially annotated\n",
    "    \n",
    "    # If we reached here, there's no feature that covers that gap region,\n",
    "    # so we have at least 50 unannotated bases.\n",
    "    return True\n",
    "\n",
    "count_filter3 = 0\n",
    "for record in records_filter2_fail:  # those that had flanking region but no transgene\n",
    "    if unannotated_region_in_flanking_interval(\n",
    "        record,\n",
    "        feat_3_names,\n",
    "        feat_5_names,\n",
    "        min_gap=50\n",
    "    ):\n",
    "        count_filter3 += 1\n",
    "\n",
    "print(f\"\\nStep 3 (Filter 3) - Among {len(records_filter2_fail)} records that have flanking regions but no Transgene or 3UTR:\")\n",
    "print(f\"Number with ≥50 unannotated bases between flanking regions: {count_filter3}\")\n",
    "\n",
    "##################################################################################\n",
    "# # CELL 4: GENBANK TRIMMING (passed Filter 1 & 2) + STORE POSITIONS\n",
    "# #         Don't alter the record ID (no \"_trimmed\" suffix).\n",
    "##################################################################################\n",
    "# from Bio import SeqIO\n",
    "# from Bio.SeqRecord import SeqRecord\n",
    "# from Bio.SeqFeature import SeqFeature, FeatureLocation\n",
    "\n",
    "# Adjust these paths if needed:\n",
    "output_file_gb = \"00-Data/feature_annotated/Repeatmasker_transgene_filtered.gb\"\n",
    "removed_file_gb= \"00-Data/feature_annotated/Repeatmasker_transgene_removed.gb\"\n",
    "\n",
    "trim_positions = {}   # { record.id : (slice_left, slice_right, strand) }\n",
    "trimmed_records = []\n",
    "removed_records = []\n",
    "\n",
    "for record in records_filter2_pass:\n",
    "    best_pair = get_best_flanking_pair(record, feat_3_names, feat_5_names)\n",
    "    if not best_pair:\n",
    "        # Theoretically shouldn't happen if it passed Filter 1,\n",
    "        # but we'll handle it just in case:\n",
    "        removed_records.append(record)\n",
    "        continue\n",
    "\n",
    "    three_feat, five_feat = best_pair\n",
    "    slice_left = min(three_feat.location.start, five_feat.location.start)\n",
    "    slice_right = max(three_feat.location.end, five_feat.location.end)\n",
    "    slice_strand = three_feat.strand  # same as five_feat.strand\n",
    "\n",
    "    # Store the coords for FASTQ trimming (Cell 5, presumably)\n",
    "    trim_positions[record.id] = (slice_left, slice_right, slice_strand)\n",
    "\n",
    "    # Create trimmed GenBank record (optional):\n",
    "    sub_seq = record.seq[slice_left : slice_right]\n",
    "\n",
    "    # Reverse-complement if negative strand\n",
    "    if slice_strand == -1:\n",
    "        sub_seq = sub_seq.reverse_complement()\n",
    "\n",
    "    # Keep the same ID:\n",
    "    new_record = SeqRecord(\n",
    "        sub_seq,\n",
    "        id=record.id,\n",
    "        name=record.name,\n",
    "        description=(\n",
    "            f\"Region {slice_left}-{slice_right} from original {record.id}; \"\n",
    "            f\"strand={slice_strand}\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Adjust features\n",
    "    adjusted_features = []\n",
    "    length_sub = slice_right - slice_left\n",
    "\n",
    "    for feat in record.features:\n",
    "        f_start = feat.location.start\n",
    "        f_end   = feat.location.end\n",
    "\n",
    "        # Keep only if fully within the slice\n",
    "        if f_start >= slice_left and f_end <= slice_right:\n",
    "            new_start = f_start - slice_left\n",
    "            new_end   = f_end   - slice_left\n",
    "            new_strand = feat.strand\n",
    "\n",
    "            if slice_strand == -1:\n",
    "                # Flip for reverse complement\n",
    "                tmp_start = length_sub - new_end\n",
    "                tmp_end   = length_sub - new_start\n",
    "                new_start, new_end = tmp_start, tmp_end\n",
    "                # Flip strand if it's 1 or -1\n",
    "                if new_strand in (1, -1):\n",
    "                    new_strand = -new_strand\n",
    "\n",
    "            new_location = FeatureLocation(new_start, new_end, strand=new_strand)\n",
    "            new_feat = SeqFeature(new_location, type=feat.type, qualifiers=feat.qualifiers)\n",
    "            adjusted_features.append(new_feat)\n",
    "\n",
    "    new_record.features = adjusted_features\n",
    "    # Copy certain annotations if desired\n",
    "    new_record.annotations[\"molecule_type\"] = record.annotations.get(\"molecule_type\",\"DNA\")\n",
    "\n",
    "    trimmed_records.append(new_record)\n",
    "\n",
    "#\n",
    "# IMPORTANT: Write out the trimmed and removed records AFTER this loop.\n",
    "#\n",
    "\n",
    "# Write trimmed records\n",
    "if trimmed_records:\n",
    "    with open(output_file_gb, \"w\") as handle:\n",
    "        SeqIO.write(trimmed_records, handle, \"genbank\")\n",
    "\n",
    "# Write removed records\n",
    "if removed_records:\n",
    "    with open(removed_file_gb, \"w\") as handle:\n",
    "        SeqIO.write(removed_records, handle, \"genbank\")\n",
    "\n",
    "print(\"Trimming done.\")\n",
    "print(f\"Trimmed records: {len(trimmed_records)}\")\n",
    "print(f\"Removed records: {len(removed_records)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trimming of the FastQ File "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the length of the Feature Annotation and trimm the FastQ File accordingly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 (FASTQ trimming) - Trimmed reads: 846\n",
      "Output FASTQ file: 00-Data/samples_filtered/sample-cutadapt_sup-filtered-trimmed.fastq\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "# CELL 5: FASTQ TRIMMING BASED ON STORED (ID, START, END, STRAND), \n",
    "#         KEEPING THE SAME READ IDs.\n",
    "#######################################################################\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "input_file_fastq  = \"00-Data/samples_filtered/sample-cutadapt_sup-filtered.fastq\"\n",
    "output_file_fastq = \"00-Data/samples_filtered/sample-cutadapt_sup-filtered-trimmed.fastq\"\n",
    "\n",
    "trimmed_fastq_records = []\n",
    "\n",
    "with open(input_file_fastq, \"r\") as in_handle:\n",
    "    for fastq_record in SeqIO.parse(in_handle, \"fastq\"):\n",
    "        # If we have trimming info for this read\n",
    "        if fastq_record.id in trim_positions:\n",
    "            slice_left, slice_right, slice_strand = trim_positions[fastq_record.id]\n",
    "\n",
    "            # Ensure slice_right does not exceed read length\n",
    "            read_len = len(fastq_record.seq)\n",
    "            if slice_right > read_len:\n",
    "                slice_right = read_len  # or skip if out-of-bounds\n",
    "\n",
    "            # Slice the sequence and quality\n",
    "            trimmed_seq = fastq_record.seq[slice_left : slice_right]\n",
    "            trimmed_qual = fastq_record.letter_annotations[\"phred_quality\"][slice_left : slice_right]\n",
    "\n",
    "            # Reverse complement if strand is -1\n",
    "            if slice_strand == -1:\n",
    "                trimmed_seq = trimmed_seq.reverse_complement()\n",
    "                trimmed_qual = trimmed_qual[::-1]\n",
    "\n",
    "            # Create a new FASTQ record with the same ID\n",
    "            new_fastq_record = SeqRecord(\n",
    "                trimmed_seq,\n",
    "                id=fastq_record.id,   # same ID\n",
    "                name=fastq_record.name,\n",
    "                description=fastq_record.description + f\" [trimmed {slice_left}-{slice_right}]\"\n",
    "            )\n",
    "            new_fastq_record.letter_annotations[\"phred_quality\"] = trimmed_qual\n",
    "\n",
    "            trimmed_fastq_records.append(new_fastq_record)\n",
    "\n",
    "# Write the trimmed FASTQ\n",
    "with open(output_file_fastq, \"w\") as out_handle:\n",
    "    SeqIO.write(trimmed_fastq_records, out_handle, \"fastq\")\n",
    "\n",
    "print(f\"Step 5 (FASTQ trimming) - Trimmed reads: {len(trimmed_fastq_records)}\")\n",
    "print(f\"Output FASTQ file: {output_file_fastq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Minimap Alignment with trimmed Fastq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[M::mm_idx_gen::0.000*3.97] collected minimizers\n",
      "[M::mm_idx_gen::0.001*3.34] sorted minimizers\n",
      "[M::main::0.001*3.08] loaded/built the index for 1 target sequence(s)\n",
      "[M::mm_mapopt_update::0.001*2.98] mid_occ = 10\n",
      "[M::mm_idx_stat] kmer size: 15; skip: 10; is_hpc: 0; #seq: 1\n",
      "[M::mm_idx_stat::0.001*2.89] distinct minimizers: 435 (99.31% are singletons); average occurrences: 1.011; average spacing: 5.232; total length: 2302\n",
      "[M::worker_pipeline::0.134*2.84] mapped 846 sequences\n",
      "[M::main] Version: 2.28-r1209\n",
      "[M::main] CMD: minimap2 -a --splice --MD --eqx 00-Data/references/ref_400nt_flanking_Transgene.fasta 00-Data/samples_filtered/sample-cutadapt_sup-filtered-trimmed.fastq\n",
      "[M::main] Real time: 0.134 sec; CPU: 0.380 sec; Peak RSS: 0.017 GB\n"
     ]
    }
   ],
   "source": [
    "# repeat alignment with trimmed reads, reduce minimal chain score requirement \n",
    "# -m 15\n",
    "\n",
    "!minimap2 -a --splice --MD --eqx 00-Data/references/ref_400nt_flanking_Transgene.fasta 00-Data/samples_filtered/sample-cutadapt_sup-filtered-trimmed.fastq | samtools view -h -F 4 -o 00-Data/filtered_aligned/trimmed.sam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimap Alignment with full length FASTQ and greater Endbonus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[M::mm_idx_gen::0.001*2.19] collected minimizers\n",
      "[M::mm_idx_gen::0.001*2.42] sorted minimizers\n",
      "[M::main::0.001*2.41] loaded/built the index for 1 target sequence(s)\n",
      "[M::mm_mapopt_update::0.001*2.33] mid_occ = 30\n",
      "[M::mm_idx_stat] kmer size: 15; skip: 10; is_hpc: 0; #seq: 1\n",
      "[M::mm_idx_stat::0.001*2.26] distinct minimizers: 1844 (97.07% are singletons); average occurrences: 1.075; average spacing: 5.331; total length: 10572\n",
      "[M::worker_pipeline::77.346*2.97] mapped 26386 sequences\n",
      "[M::main] Version: 2.28-r1209\n",
      "[M::main] CMD: minimap2 -a --splice --end-bonus 1000 --MD --eqx 00-Data/references/ref_crRNA_Transgene.fasta 00-Data/samples_filtered/sample-cutadapt_sup-filtered.fastq\n",
      "[M::main] Real time: 77.347 sec; CPU: 229.968 sec; Peak RSS: 0.988 GB\n"
     ]
    }
   ],
   "source": [
    "!minimap2 -a --splice --end-bonus 1000 --MD --eqx 00-Data/references/ref_crRNA_Transgene.fasta 00-Data/samples_filtered/sample-cutadapt_sup-filtered.fastq  | samtools view -h -F 4 -o 00-Data/filtered_aligned/crRNA.sam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RepeatMaster2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
